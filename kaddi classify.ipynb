{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    kaddi_names = [item[8:-1] for item in sorted(glob(path+\"/*/\"))]\n",
    "    data = load_files(path)\n",
    "    kaddi_files = np.array(data['filenames'])\n",
    "    kaddi_targets = np_utils.to_categorical(np.array(data['target']), len(kaddi_names))\n",
    "    return kaddi_files, kaddi_targets, kaddi_names;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaddi_files, kaddi_targets, kaddi_names = load_dataset('Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Dataset/bad_sticks/b655.jpg', 'Dataset/good_sticks/g219.jpg',\n",
       "       'Dataset/good_sticks/g1877.jpg', ...,\n",
       "       'Dataset/good_sticks/g1373.jpg', 'Dataset/good_sticks/g2231.jpg',\n",
       "       'Dataset/good_sticks/g2344.jpg'], \n",
       "      dtype='<U29')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaddi_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       ..., \n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaddi_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bad_sticks', 'good_sticks']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaddi_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 total kaddi categories.\n",
      "There are 4500 total kaddi images.\n",
      "\n",
      "There are 2880 training kaddi images.\n",
      "There are 720 validation kaddi images.\n",
      "There are 900 test kaddi images.\n"
     ]
    }
   ],
   "source": [
    "train_files, test_files, train_targets, test_targets = train_test_split(kaddi_files, kaddi_targets, test_size=0.2, random_state=0)\n",
    "\n",
    "train_files, valid_files, train_targets, valid_targets = train_test_split(train_files, train_targets, test_size=0.2, random_state=0)\n",
    "\n",
    "# # print statistics about the dataset\n",
    "print('There are %d total kaddi categories.' % len(kaddi_names))\n",
    "print('There are %s total kaddi images.\\n' % len(kaddi_files))\n",
    "print('There are %d training kaddi images.' % len(train_files))\n",
    "print('There are %d validation kaddi images.' % len(valid_files))\n",
    "print('There are %d test kaddi images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the Data\n",
    "\n",
    "When using TensorFlow as backend, Keras CNNs require a 4D array (which we'll also refer to as a 4D tensor) as input, with shape\n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, \\text{rows}, \\text{columns}, \\text{channels}),\n",
    "$$\n",
    "\n",
    "where `nb_samples` corresponds to the total number of images (or samples), and `rows`, `columns`, and `channels` correspond to the number of rows, columns, and channels for each image, respectively.  \n",
    "\n",
    "The `path_to_tensor` function below takes a string-valued file path to a color image as input and returns a 4D tensor suitable for supplying to a Keras CNN.  The function first loads the image and resizes it to a square image that is $224 \\times 224$ pixels.  Next, the image is converted to an array, which is then resized to a 4D tensor.  In this case, since we are working with color images, each image has three channels.  Likewise, since we are processing a single image (or sample), the returned tensor will always have shape\n",
    "\n",
    "$$\n",
    "(1, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "The `paths_to_tensor` function takes a numpy array of string-valued image paths as input and returns a 4D tensor with shape \n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "Here, `nb_samples` is the number of samples, or number of images, in the supplied array of image paths.  It is best to think of `nb_samples` as the number of 3D tensors (where each 3D tensor corresponds to a different image) in your dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224), grayscale=True)\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2880/2880 [00:36<00:00, 78.20it/s]\n",
      "100%|██████████| 720/720 [00:08<00:00, 81.40it/s]\n",
      "100%|██████████| 900/900 [00:10<00:00, 81.82it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Convolution2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "def getMyModel():\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters=16, kernel_size=3,padding='same', activation='relu', input_shape=train_tensors.shape[1:]))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Convolution2D(32,2,padding='same',activation='relu'))\n",
    "    model.add(MaxPooling2D(2))\n",
    "#     model.add(Convolution2D(64,2,padding='same',activation='relu'))\n",
    "#     model.add(MaxPooling2D(2))\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dense(len(kaddi_names), activation='softmax'))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 224, 224, 16)      160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 112, 112, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 112, 112, 32)      2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 56, 56, 32)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_4 ( (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 2,306.0\n",
      "Trainable params: 2,306.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = getMyModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.\n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2880 samples, validate on 720 samples\n",
      "Epoch 1/12\n",
      "2860/2880 [============================>.] - ETA: 0s - loss: 0.5956 - acc: 0.7259Epoch 00000: val_loss improved from inf to 0.59985, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2880/2880 [==============================] - 77s - loss: 0.5947 - acc: 0.7267 - val_loss: 0.5999 - val_acc: 0.7153\n",
      "Epoch 2/12\n",
      "2860/2880 [============================>.] - ETA: 0s - loss: 0.5897 - acc: 0.7273Epoch 00001: val_loss improved from 0.59985 to 0.59634, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2880/2880 [==============================] - 73s - loss: 0.5902 - acc: 0.7267 - val_loss: 0.5963 - val_acc: 0.7153\n",
      "Epoch 3/12\n",
      "2860/2880 [============================>.] - ETA: 0s - loss: 0.5876 - acc: 0.7259Epoch 00002: val_loss improved from 0.59634 to 0.59306, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2880/2880 [==============================] - 72s - loss: 0.5869 - acc: 0.7267 - val_loss: 0.5931 - val_acc: 0.7153\n",
      "Epoch 4/12\n",
      "2860/2880 [============================>.] - ETA: 0s - loss: 0.5829 - acc: 0.7262Epoch 00003: val_loss improved from 0.59306 to 0.58972, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2880/2880 [==============================] - 72s - loss: 0.5824 - acc: 0.7267 - val_loss: 0.5897 - val_acc: 0.7153\n",
      "Epoch 5/12\n",
      "2860/2880 [============================>.] - ETA: 0s - loss: 0.5737 - acc: 0.7248Epoch 00004: val_loss improved from 0.58972 to 0.57405, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2880/2880 [==============================] - 73s - loss: 0.5730 - acc: 0.7257 - val_loss: 0.5741 - val_acc: 0.7153\n",
      "Epoch 6/12\n",
      "2860/2880 [============================>.] - ETA: 0s - loss: 0.5558 - acc: 0.7329Epoch 00005: val_loss improved from 0.57405 to 0.55031, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2880/2880 [==============================] - 72s - loss: 0.5555 - acc: 0.7330 - val_loss: 0.5503 - val_acc: 0.7222\n",
      "Epoch 7/12\n",
      "2860/2880 [============================>.] - ETA: 0s - loss: 0.5276 - acc: 0.7524Epoch 00006: val_loss improved from 0.55031 to 0.51272, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2880/2880 [==============================] - 73s - loss: 0.5276 - acc: 0.7524 - val_loss: 0.5127 - val_acc: 0.7653\n",
      "Epoch 8/12\n",
      "2860/2880 [============================>.] - ETA: 0s - loss: 0.5133 - acc: 0.7727Epoch 00007: val_loss did not improve\n",
      "2880/2880 [==============================] - 72s - loss: 0.5117 - acc: 0.7740 - val_loss: 0.5149 - val_acc: 0.7458\n",
      "Epoch 9/12\n",
      "2860/2880 [============================>.] - ETA: 0s - loss: 0.4985 - acc: 0.7811Epoch 00008: val_loss improved from 0.51272 to 0.48969, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2880/2880 [==============================] - 74s - loss: 0.4986 - acc: 0.7809 - val_loss: 0.4897 - val_acc: 0.7639\n",
      "Epoch 10/12\n",
      "2860/2880 [============================>.] - ETA: 0s - loss: 0.4681 - acc: 0.7983Epoch 00009: val_loss improved from 0.48969 to 0.46849, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2880/2880 [==============================] - 78s - loss: 0.4678 - acc: 0.7983 - val_loss: 0.4685 - val_acc: 0.7833\n",
      "Epoch 11/12\n",
      "2860/2880 [============================>.] - ETA: 0s - loss: 0.4496 - acc: 0.8101Epoch 00010: val_loss improved from 0.46849 to 0.46093, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2880/2880 [==============================] - 78s - loss: 0.4492 - acc: 0.8101 - val_loss: 0.4609 - val_acc: 0.7861\n",
      "Epoch 12/12\n",
      "2860/2880 [============================>.] - ETA: 0s - loss: 0.4416 - acc: 0.8108Epoch 00011: val_loss improved from 0.46093 to 0.41992, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "2880/2880 [==============================] - 78s - loss: 0.4419 - acc: 0.8108 - val_loss: 0.4199 - val_acc: 0.8347\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x118de4c88>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "### TODO: specify the number of epochs that you would like to use to train the model.\n",
    "\n",
    "epochs = 12\n",
    "\n",
    "### Do NOT modify the code below this line.\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('saved_models/weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 84.1111%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "kaddi_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(kaddi_predictions)==np.argmax(test_targets, axis=1))/len(kaddi_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('a.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "dog-project",
   "language": "python",
   "name": "dog-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
